{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import unicode_literals, print_function, division\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import os\n",
    "import re\n",
    "import random\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "SOS_token = 0\n",
    "EOS_token = 0\n",
    "MAX_LENGTH = 20\n",
    "\n",
    "class Lang: # 딕셔너리를 만들기 위한 클래스\n",
    "    def __init__(self): # 단어의 인덱스를 저장하기 위한 컨테이너를 초기화\n",
    "        self.word2index = {}\n",
    "        self.word2count = {}\n",
    "        self.index2word = {0: \"SOS\", 1: \"EOS\"} # SOS(Start of Sequence): 문장의 시작, EOS(End of Sequence): 문장의 끝\n",
    "        self.n_words = 2 # SOS와 EOS에 대한 카운트\n",
    "\n",
    "    def addSentence(self, sentence): # 문장을 단어 단위로 분리한 후 컨테이너(word)에 추가\n",
    "        for word in sentence.split(' '):\n",
    "            self.addWord(word)\n",
    "    \n",
    "    def addWord(self, word): # 컨테이너에 단어가 없다면 추가되고, 있다면 카운트를 업데이트\n",
    "        if word not in self.word2index:\n",
    "            self.word2index[word] = self.n_words\n",
    "            self.word2count[word] = 1\n",
    "            self.index2word[self.n_words] = word\n",
    "            self.n_words += 1\n",
    "        else: \n",
    "            self.word2count[word] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalizeString(df, lang):\n",
    "    sentence = df[lang].str.lower() # 소문자로 변환\n",
    "    sentence = sentence.str.replace('[A-Za-z\\s]+' '') # a-z, A-Z, ..., ?, ! 등을 제외하고 모두 공백으로 변환\n",
    "    sentence = sentence.str.normalize('NFD') # 유니코드 정규화 방식\n",
    "    sentence = sentence.str.encode('ascii', errors = 'ignore').str.decode('utf-8') # Unicode를 ASCII로 변환\n",
    "    return sentence\n",
    "\n",
    "def read_sentence(df, lang1, lang2):\n",
    "    sentence1 = normalizeString(df, lang1) # 데이터셋의 첫 번째 열(영어)\n",
    "    sentence2 = normalizeString(df, lang2) # 데이터셋의 두 번째 열(프랑스어)\n",
    "    return sentence1, sentence2\n",
    "\n",
    "def read_file(loc, lang1, lang2):\n",
    "    df = pd.read_csv(loc, delimiter = '\\t', header = None, names = [lang1, lang2])\n",
    "    return df\n",
    "\n",
    "def process_data(lang1, lang2):\n",
    "    df = read_file('D:/kim/kim/DP/data/%s-%s.txt'%(lang1, lang2), lang1, lang2) # 데이터셋 불러오기\n",
    "    sentence1, sentence2 = read_sentence(df, lang1, lang2)\n",
    "\n",
    "    input_lang = Lang()\n",
    "    output_lang = Lang()\n",
    "    pairs = []\n",
    "    for i in range(len(df)):\n",
    "        if len(sentence1[i].split(' ')) < MAX_LENGTH and len(sentence2[i].split(' ')) < MAX_LENGTH:\n",
    "            full = [sentence1[i], sentence2[i]] # 첫 번째와 두 번째 열을 합쳐서 저장\n",
    "            input_lang.addSentence(sentence1[i]) # 입력으로 영어를 사용\n",
    "            output_lang.addSentence(sentence2[i]) # 출력으로 프랑스어를 사용\n",
    "            pairs.append(full) # pairs에는 입력과 출력이 합쳐진 것을 사용\n",
    "\n",
    "    return input_lang, output_lang, pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def indexesFromSentence(lang, sentence): # 문장을 단어로 분리하고 인덱스를 반환\n",
    "    return [lang.word2index[word] for word in sentence.split(' ')]\n",
    "\n",
    "def tensorFromSentence(lang, sentence): # 딕셔너리에서 단어에 대한 인덱스를 가져오고 문장 끝에 토큰을 추가\n",
    "    indexes = indexesFromSentence(lang, sentence)\n",
    "    indexes.append(EOS_token)\n",
    "    return torch.tensor(indexes, dtype = torch.long, device = device).view(-1, 1)\n",
    "\n",
    "def tensorsFromPair(input_lang, output_lang, pair): # 입력과 출력 문장을 텐서로 변환하여 반환\n",
    "    input_tensor = tensorFromSentence(input_lang, pair[0])\n",
    "    target_tensor = tensorFromSentence(output_lang, pair[1])\n",
    "    return (input_tensor, target_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, embbed_dim, num_layers):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.input_dim = input_dim # 인코더에서 사용할 입력층\n",
    "        self.embbed_dim = embbed_dim # 인코더에서 사용할 임베딩 계층\n",
    "        self.hidden_dim = hidden_dim # 인코더에서 사용할 은닉층(이전 은닉층)\n",
    "        self.num_layers = num_layers # 인코더에서 사용할 GRU의 계층 개수\n",
    "        self.embedding = nn.Embedding(input_dim, self.embbed_dim) # 임베딩 계층 초기화\n",
    "        self.gru = nn.GRU(self.embbed_dim, self.hidden_dim, num_layers = self.num_layers) # 임베딩 차원, 은닉층 차원, GRU의 계층 개수를 이용하여 GRU 게층을 초기화\n",
    "\n",
    "    def forward(self, src):\n",
    "        embedded = self.embedding(src).view(1, 1, -1) # 임베딩 처리\n",
    "        outputs, hidden = self.gru(embedded) # 임베딩 결과를 GRU 모델에 적용\n",
    "        return outputs, hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    def __init__(self, output_dim, hidden_dim, embbed_dim, num_layers):\n",
    "        super(Decoder, self).__init__()\n",
    "\n",
    "        self.embbed_dim = embbed_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.output_dim = output_dim\n",
    "        self.num_layers = num_layers\n",
    "\n",
    "        self.embedding = nn.Embedding(output_dim, self.embbed_dim) # 임베딩 계층 초기화\n",
    "        self.gru = nn.GRU(self.embbed_dim, self.hidden_dim, num_layers = self.num_layers) # GRU 계층 초기화\n",
    "        self.out = nn.Linear(self.hidden_dim, output_dim) # 선형 계층 초기화\n",
    "        self.softmax = nn.LogSoftmax(dim = 1)\n",
    "\n",
    "    def forward(self, input, hidden):\n",
    "        input = input.view(1, -1) # 입력을 (1, 배치 크기)로 변경\n",
    "        embedded = F.relu(self.embedding(input))\n",
    "        output, hidden = self.gru(embedded, hidden)\n",
    "        prediction = self.softmax(self.out(output[0]))\n",
    "        return prediction, hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
